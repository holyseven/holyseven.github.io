<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="holyseven's blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="holyseven's blog">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="holyseven's blog">
<meta name="twitter:description">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> holyseven's blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">holyseven's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/13/2016-05-03-ReadingPaper-FCN/" itemprop="url">
                  Fully Convolutional Networks#PaperReading#
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-13T01:42:11+02:00" content="2016-05-13">
              2016-05-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/13/2016-05-03-ReadingPaper-FCN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/13/2016-05-03-ReadingPaper-FCN/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>KeyWords</strong>: semantic segmentation, fully convolutional</p>
<hr>
<p><strong>Paper Author</strong>: Jonathan Long, Evan Shlhamer Trevor Darrell</p>
<p><strong>Research Institu</strong>: Berkeley</p>
<p><strong>Database</strong>: VOC, NYUDv2, SIFT Flow</p>
<p><strong>GPU &amp; timing</strong>: NVIDIA K40c GPU</p>
<p><strong>Paper Link</strong>: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html" target="_blank" rel="external">cvpr15</a></p>
<hr>
<h1 id="3-FCN"><a href="#3-FCN" class="headerlink" title="3. FCN"></a><strong>3. FCN</strong></h1><p>直接跳到第三章来，不知道前两章在讲什么，看得我好困啊。</p>
<p>3.1首先说了把fc-layer变成1X1的convnet，这样就可以接受不同size的image了，只是输出的尺寸是根据输入的变化而变化的。文中的有些话一直不懂：</p>
<blockquote>
<p>When these receptive fields overlap significantly, both feedforward computation and backpropagation are much more efficient when computed layer-by-layer over an entire image instead of independently patch-by-patch.</p>
<p>Furthermore, while the resulting maps are equivalent to the evaluation of the original net on particular input patches, the computation is highly amortized over the overlapping regions of those patches.</p>
</blockquote>
<p><strong>问题1</strong>： 为什么因为receptive field overlap，计算量就会大大降低？是因为SPP-layer吗？但是代码中并没有使用这种layer啊？不懂。。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/13/2016-05-10-ReadingPaper-C3D/" itemprop="url">
                  Learning Spatiotemporal Features with 3D Convolutional Networks#PaperReading#
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-13T01:40:27+02:00" content="2016-05-13">
              2016-05-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/13/2016-05-10-ReadingPaper-C3D/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/13/2016-05-10-ReadingPaper-C3D/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>KeyWords</strong>: 3d convnet, action recognition.</p>
<hr>
<p><strong>Paper Author</strong>: Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri</p>
<p><strong>Research Institu</strong>: Facebook AI Reasearch, Dartmouth College</p>
<p><strong>Database</strong>: UCF101, Sport1m, I380K(internal database)</p>
<p><strong>GPU &amp; timing</strong>: </p>
<p><strong>Paper Link</strong>: <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html" target="_blank" rel="external">iccv15</a></p>
<hr>
<h1 id="3-3D-ConvNets"><a href="#3-3D-ConvNets" class="headerlink" title="3. 3D ConvNets"></a><strong>3. 3D ConvNets</strong></h1><p>又是从第三章开始。3D convnet就不叙述了。文中把视频截断成16-frame的clips，所以input就变成了3x16x128x171。然后在随机crop成3x16x112x112的图片，可以增加数据量，但是这种数据量的增加我觉得需要打折扣，并不能完全乘以16x59这样计算。我觉得这里是讲的train from scratch on UCF101，所以用的网络比较小，5个卷积层，3个fc，2048，2048，101。batch size = 30, lr = 0.003， 4 epochs之后除以10， 总共16个epochs。</p>
<p>然后讨论了depth应该取多少，最后好像是3最好。但是train from scratch最多到达45%的准确率（clip-based）。</p>
<p><strong>问题1</strong><br>但是可以从图中看到，在epoch 3,4的时候，accuracy是降低的，然后过了4，又升上去了，好奇怪，正常吗?</p>
<p>然后就开始正常的训练了，实际上作者是用的他们自己的数据库I380K训练的，这个好像是用了8个convnet，5个pooling，fc4096，4096，101。然后在这个数据库的基础上再fine-tuning。好像引用中[29]的准确律在sport1m上比C3D还要高，文中说120-frame的clips不能直接跟文中的16-frame的准确率比较。</p>
<p>文中的准确率的图表真让人看得眼花缭乱。 Table 2里面说的C3D(trained from scratch) 44.9, 60.0, 84.4, C3D(fine-tuned from I380K pre-trained model)在单个clip上的是46.1，单个video上的是61.1，单个video@5 is 85.2。但是再第4章里有又说，</p>
<blockquote>
<p>C3D with 3 nets boosts the accuracy to 85.2% with the dimension is increased to 12,288.</p>
</blockquote>
<p>In table 3, C3D(1 net) + SVM: 82.3, C3D(3 nets) + SVM: 85.2</p>
<p>唉，导师说我这篇文章没有仔细看明白，感觉看明白了也做不出来啊。首先因为他用了自己的数据库。然后不知道如果从sport1m入手的准确率怎么样。反正现在用ucf101，准确率最多到37，38%左右。离作者说的45%还差一截，不过我觉得差的也只是其中的一些参数的调整了。但是这个45%，是建立在training data的准确率是接近100%的，这样难道不是overfitting吗？</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/13/2016-04-29-reinstall-ubuntu/" itemprop="url">
                  Reinstall Ubuntu 14.04 and Teamviewer, jekyll
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-13T01:40:01+02:00" content="2016-05-13">
              2016-05-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/13/2016-04-29-reinstall-ubuntu/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/13/2016-04-29-reinstall-ubuntu/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>No idea why all are removed by a command, including ubuntu-desktop etc.</p>
<hr>
<h1 id="Install-Ubuntu14-04-and-all"><a href="#Install-Ubuntu14-04-and-all" class="headerlink" title="Install Ubuntu14.04 and all"></a>Install Ubuntu14.04 and all</h1><p>So reinstall ubuntu14.04 from scratch and configure all. For avoiding this situation next time, I’ve noted some installation in my <a href="https://github.com/holyseven/configuration_files/blob/master/run.sh" target="_blank" rel="external">git</a>. Luckily, I have copied the modified caffe in another disk.</p>
<p>Nothing difficult and strange I’ve met. So just following these steps. Maybe there is something about backup but I don’t want to test it any more. Hope I will never touch these.</p>
<hr>
<h1 id="Teamviewer"><a href="#Teamviewer" class="headerlink" title="Teamviewer"></a>Teamviewer</h1><p>It’s about the remote access from distance, like a functinality in qq. Just download a quicksupport <a href="http://www.teamviewer.com/en/download/previous-versions/#version11" target="_blank" rel="external">here</a> in a computer which you want to control.</p>
<p>And download a teamviewer deb (32 or 64 is not important, I think. My computer is 64-bit but I’m using the 32-bit version). Install it by </p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">sudo</span> <span class="selector-tag">dkpg</span> <span class="selector-tag">-i</span> <span class="selector-tag">teamviewer_11</span><span class="selector-class">.0</span><span class="selector-class">.57095_i386</span><span class="selector-class">.deb</span></span><br></pre></td></tr></table></figure>
<p>There will be some errors. Then</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -f <span class="keyword">install</span></span><br></pre></td></tr></table></figure>
<p>This is the command that removes all. But this time no problem. Maybe something wrong at first time, maybe I taped sudo apt-get install -f. But I didn’t figure out the diffrence.</p>
<hr>
<h1 id="Install-Jekyll"><a href="#Install-Jekyll" class="headerlink" title="Install Jekyll"></a>Install Jekyll</h1><p>jekyll is for locally compiling github pages and showing them. Before I’ve already installed it but after the reinstallation of ubuntu, all gone. So I have to reinstall it.</p>
<p>Look at <a href="https://help.github.com/articles/setting-up-your-github-pages-site-locally-with-jekyll/" target="_blank" rel="external">this</a>. And it needs to compile ruby 2.3, at the end of <a href="https://www.ruby-lang.org/en/documentation/installation/" target="_blank" rel="external">this page</a>. When install ruby, pop out the problem of openssl. I follow these: </p>
<figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install libssl-<span class="built_in">dev</span> libreadline-<span class="built_in">dev</span></span><br></pre></td></tr></table></figure>
<p>and in the directory of ruby source:  </p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pushd <span class="keyword">ext/openssl</span><br><span class="line"></span>ruby <span class="keyword">extconf.rb</span><br><span class="line"></span>make &amp;&amp; sudo make <span class="keyword">install</span><br><span class="line"></span>popd</span><br></pre></td></tr></table></figure>
<p>Then sudo make clean and sudo make install. So ruby2.3 is installed with openssl.</p>
<p>Then</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ~/phdwork/myblogs</span><br><span class="line">sudo apt-get <span class="keyword">install</span> nodejs <span class="comment"># JavaScript-runtime</span></span><br><span class="line">bundle <span class="keyword">install</span> <span class="comment"># This will install jekyll, because I have already have a Gemfile in this directory</span></span><br></pre></td></tr></table></figure>
<hr>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/13/2016-04-27-caffe-code-convert-imageset/" itemprop="url">
                  Caffe Code Reading - Convert Image Set
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-13T01:39:38+02:00" content="2016-05-13">
              2016-05-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/13/2016-04-27-caffe-code-convert-imageset/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/13/2016-04-27-caffe-code-convert-imageset/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>重写了tools/convert_imageset。还不算太难，把video里读出来的Mat用cv::merge就可以连成一个48个channels的datum了。所以这个frame的长度预先定义好了。。就这个而言的话，并不是很好。居然忘记了set_label，还好在训练开始的时候就看到了loss都直接为0了，然后发现这个问题，最后加上了，然后重新生成lmdb，还要重新拷贝到服务器上去，重新训练。</p>
<p>然后使用了reshape layer增加了一个dimension，应该是没有把数据弄乱。</p>
<p>两个GPU互相之间的通信应该会占用一定的内存，或者在读取数据的时候会有一定的mutex机制，所以之前数据是以单个文件存在的时候，GPU的速度也受影响。但是使用lmdb这种数据库的话，读取速度特别快，即使GPU内存很高也没有问题，两个使用率都能达到99%。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/13/2016-04-26-caffe-code-reading-layer/" itemprop="url">
                  Caffe Code Reading - blob and layer
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-13T01:39:25+02:00" content="2016-05-13">
              2016-05-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/13/2016-04-26-caffe-code-reading-layer/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/13/2016-04-26-caffe-code-reading-layer/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>看blob.hpp和layer.hpp等。</p>
<hr>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//layer.hpp</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span><br><span class="line">   * @brief Implements common layer setup functionality.</span><br><span class="line">   *</span><br><span class="line">   * @param bottom the preshaped input blobs</span><br><span class="line">   * @param top</span><br><span class="line">   *     the allocated but unshaped output blobs, to be shaped by Reshape</span><br><span class="line">   *</span><br><span class="line">   * Checks that the number of bottom and top blobs is correct.</span><br><span class="line">   * Calls LayerSetUp to do special layer setup for individual layer types,</span><br><span class="line">   * followed by Reshape to set up sizes of top blobs and internal buffers.</span><br><span class="line">   * Sets up the loss weight multiplier blobs for any non-zero loss weights.</span><br><span class="line">   * This method may not be overridden.</span><br><span class="line">   */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">SetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">    InitMutex();<span class="comment">//在layer.cpp里实现了</span></span><br><span class="line">    CheckBlobCounts(bottom, top);<span class="comment">//里面有函数没有实现，&#123;ExactNum,Min,Max&#125;&#123;Bottom,Top&#125;Blobs()</span></span><br><span class="line">    LayerSetUp(bottom, top);<span class="comment">//这个感觉就完全是要自己实现了</span></span><br><span class="line">    Reshape(bottom, top);<span class="comment">//同上</span></span><br><span class="line">    SetLossWeights(top);<span class="comment">//好像不用自己实现了</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//base_conv_layer.hpp</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Abstract base class that factors out the BLAS code common to ConvolutionLayer and DeconvolutionLayer.</span></span><br><span class="line"><span class="comment">// 意思是说这是convnet和deconvnet中相同的blas代码？</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> BaseConvolutionLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt; &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MinBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; return <span class="number">1</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MinTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; return <span class="number">1</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">EqualNumBottomTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; return <span class="literal">true</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">  <span class="comment">/// @brief The spatial dimensions of a filter kernel.</span></span><br><span class="line">  Blob&lt;<span class="keyword">int</span>&gt; kernel_shape_;</span><br><span class="line">  <span class="comment">/// @brief The spatial dimensions of the stride.</span></span><br><span class="line">  Blob&lt;<span class="keyword">int</span>&gt; stride_;</span><br><span class="line">  <span class="comment">/// @brief The spatial dimensions of the padding.</span></span><br><span class="line">  Blob&lt;<span class="keyword">int</span>&gt; pad_;</span><br><span class="line">  <span class="comment">/// @brief The spatial dimensions of the dilation.</span></span><br><span class="line">  Blob&lt;<span class="keyword">int</span>&gt; dilation_;</span><br><span class="line">  <span class="comment">/// @brief The spatial dimensions of the convolution input.</span></span><br><span class="line">  Blob&lt;<span class="keyword">int</span>&gt; conv_input_shape_;</span><br><span class="line">  <span class="comment">/// @brief The spatial dimensions of the col_buffer.</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; col_buffer_shape_;</span><br><span class="line">  <span class="comment">/// @brief The spatial dimensions of the output.</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; output_shape_;</span><br><span class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;* bottom_shape_;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> num_spatial_axes_;</span><br><span class="line">  <span class="keyword">int</span> bottom_dim_;</span><br><span class="line">  <span class="keyword">int</span> top_dim_;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> channel_axis_;<span class="comment">//default: 1， 就是那个设置为“axis”的值，应该是从0开始。在它之前的</span></span><br><span class="line">                    <span class="comment">//layer被当做independent，在它之后的layer被当做spatial，要进行</span></span><br><span class="line">                    <span class="comment">//卷积的。见caffe.proto</span></span><br><span class="line">  <span class="keyword">int</span> num_;</span><br><span class="line">  <span class="keyword">int</span> channels_;</span><br><span class="line">  <span class="keyword">int</span> group_;</span><br><span class="line">  <span class="keyword">int</span> out_spatial_dim_;</span><br><span class="line">  <span class="keyword">int</span> weight_offset_;</span><br><span class="line">  <span class="keyword">int</span> num_output_;</span><br><span class="line">  <span class="keyword">bool</span> bias_term_;</span><br><span class="line">  <span class="keyword">bool</span> is_1x1_;</span><br><span class="line">  <span class="keyword">bool</span> force_nd_im2col_;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// base_conv_layer.cpp</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::LayerSetUp(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">   <span class="comment">// 以3d-net为例说明。BxFxCxHxW。或者Bx1xFCxHxW。batch_size, frames, channels。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Configure the kernel size, padding, stride, and inputs.</span></span><br><span class="line">  ConvolutionParameter conv_param = <span class="keyword">this</span>-&gt;layer_param_.convolution_param();</span><br><span class="line">  force_nd_im2col_ = conv_param.force_nd_im2col();</span><br><span class="line">  <span class="comment">//default: 1</span></span><br><span class="line">  channel_axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(conv_param.axis());</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> first_spatial_axis = channel_axis_ + <span class="number">1</span>;<span class="comment">//2</span></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> num_axes = bottom[<span class="number">0</span>]-&gt;num_axes();<span class="comment">//5</span></span><br><span class="line">  num_spatial_axes_ = num_axes - first_spatial_axis;<span class="comment">//5-2=3</span></span><br><span class="line">  CHECK_GE(num_spatial_axes_, <span class="number">0</span>);</span><br><span class="line">  <span class="comment">// one value in the vector: 4, not used</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bottom_dim_blob_shape(<span class="number">1</span>, num_spatial_axes_ + <span class="number">1</span>);</span><br><span class="line">  <span class="comment">//one value: max(3,1)=3</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; spatial_dim_blob_shape(<span class="number">1</span>, <span class="built_in">std</span>::max(num_spatial_axes_, <span class="number">1</span>));</span><br><span class="line">  <span class="comment">// Setup filter kernel dimensions (kernel_shape_).</span></span><br><span class="line">  kernel_shape_.Reshape(spatial_dim_blob_shape);</span><br><span class="line">  <span class="keyword">int</span>* kernel_shape_data = kernel_shape_.mutable_cpu_data();</span><br><span class="line">  <span class="keyword">if</span> (conv_param.has_kernel_h() || conv_param.has_kernel_w()) &#123; <span class="comment">// not this case</span></span><br><span class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</span><br><span class="line">        &lt;&lt; <span class="string">"kernel_h &amp; kernel_w can only be used for 2D convolution."</span>;</span><br><span class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.kernel_size_size())</span><br><span class="line">        &lt;&lt; <span class="string">"Either kernel_size or kernel_h/w should be specified; not both."</span>;</span><br><span class="line">    kernel_shape_data[<span class="number">0</span>] = conv_param.kernel_h();</span><br><span class="line">    kernel_shape_data[<span class="number">1</span>] = conv_param.kernel_w();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="comment">// repeated times of kernel_size, 3</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_kernel_dims = conv_param.kernel_size_size();</span><br><span class="line">    <span class="comment">// one kernel_size represents all</span></span><br><span class="line">    <span class="comment">// or repeated times of kernel_size must equal num_spatial_axes</span></span><br><span class="line">    CHECK(num_kernel_dims == <span class="number">1</span> || num_kernel_dims == num_spatial_axes_)</span><br><span class="line">        &lt;&lt; <span class="string">"kernel_size must be specified once, or once per spatial dimension "</span></span><br><span class="line">        &lt;&lt; <span class="string">"(kernel_size specified "</span> &lt;&lt; num_kernel_dims &lt;&lt; <span class="string">" times; "</span></span><br><span class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</span><br><span class="line">        kernel_shape_data[i] =</span><br><span class="line">            conv_param.kernel_size((num_kernel_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</span><br><span class="line">    CHECK_GT(kernel_shape_data[i], <span class="number">0</span>) &lt;&lt; <span class="string">"Filter dimensions must be nonzero."</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Setup stride dimensions (stride_).</span></span><br><span class="line">  <span class="comment">// Similar as filter kernel dimensions</span></span><br><span class="line">  stride_.Reshape(spatial_dim_blob_shape);</span><br><span class="line">  <span class="keyword">int</span>* stride_data = stride_.mutable_cpu_data();</span><br><span class="line">  <span class="keyword">if</span> (conv_param.has_stride_h() || conv_param.has_stride_w()) &#123;</span><br><span class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</span><br><span class="line">        &lt;&lt; <span class="string">"stride_h &amp; stride_w can only be used for 2D convolution."</span>;</span><br><span class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.stride_size())</span><br><span class="line">        &lt;&lt; <span class="string">"Either stride or stride_h/w should be specified; not both."</span>;</span><br><span class="line">    stride_data[<span class="number">0</span>] = conv_param.stride_h();</span><br><span class="line">    stride_data[<span class="number">1</span>] = conv_param.stride_w();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_stride_dims = conv_param.stride_size();</span><br><span class="line">    CHECK(num_stride_dims == <span class="number">0</span> || num_stride_dims == <span class="number">1</span> ||</span><br><span class="line">          num_stride_dims == num_spatial_axes_)</span><br><span class="line">        &lt;&lt; <span class="string">"stride must be specified once, or once per spatial dimension "</span></span><br><span class="line">        &lt;&lt; <span class="string">"(stride specified "</span> &lt;&lt; num_stride_dims &lt;&lt; <span class="string">" times; "</span></span><br><span class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> kDefaultStride = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</span><br><span class="line">      stride_data[i] = (num_stride_dims == <span class="number">0</span>) ? kDefaultStride :</span><br><span class="line">          conv_param.stride((num_stride_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</span><br><span class="line">      CHECK_GT(stride_data[i], <span class="number">0</span>) &lt;&lt; <span class="string">"Stride dimensions must be nonzero."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Setup pad dimensions (pad_).</span></span><br><span class="line">  pad_.Reshape(spatial_dim_blob_shape);</span><br><span class="line">  <span class="keyword">int</span>* pad_data = pad_.mutable_cpu_data();</span><br><span class="line">  <span class="keyword">if</span> (conv_param.has_pad_h() || conv_param.has_pad_w()) &#123;</span><br><span class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</span><br><span class="line">        &lt;&lt; <span class="string">"pad_h &amp; pad_w can only be used for 2D convolution."</span>;</span><br><span class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.pad_size())</span><br><span class="line">        &lt;&lt; <span class="string">"Either pad or pad_h/w should be specified; not both."</span>;</span><br><span class="line">    pad_data[<span class="number">0</span>] = conv_param.pad_h();</span><br><span class="line">    pad_data[<span class="number">1</span>] = conv_param.pad_w();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_pad_dims = conv_param.pad_size();</span><br><span class="line">    CHECK(num_pad_dims == <span class="number">0</span> || num_pad_dims == <span class="number">1</span> ||</span><br><span class="line">          num_pad_dims == num_spatial_axes_)</span><br><span class="line">        &lt;&lt; <span class="string">"pad must be specified once, or once per spatial dimension "</span></span><br><span class="line">        &lt;&lt; <span class="string">"(pad specified "</span> &lt;&lt; num_pad_dims &lt;&lt; <span class="string">" times; "</span></span><br><span class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> kDefaultPad = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</span><br><span class="line">      pad_data[i] = (num_pad_dims == <span class="number">0</span>) ? kDefaultPad :</span><br><span class="line">          conv_param.pad((num_pad_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Setup dilation dimensions (dilation_).</span></span><br><span class="line">  dilation_.Reshape(spatial_dim_blob_shape);</span><br><span class="line">  <span class="keyword">int</span>* dilation_data = dilation_.mutable_cpu_data();</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> num_dilation_dims = conv_param.dilation_size();</span><br><span class="line">  CHECK(num_dilation_dims == <span class="number">0</span> || num_dilation_dims == <span class="number">1</span> ||</span><br><span class="line">        num_dilation_dims == num_spatial_axes_)</span><br><span class="line">      &lt;&lt; <span class="string">"dilation must be specified once, or once per spatial dimension "</span></span><br><span class="line">      &lt;&lt; <span class="string">"(dilation specified "</span> &lt;&lt; num_dilation_dims &lt;&lt; <span class="string">" times; "</span></span><br><span class="line">      &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> kDefaultDilation = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</span><br><span class="line">    dilation_data[i] = (num_dilation_dims == <span class="number">0</span>) ? kDefaultDilation :</span><br><span class="line">                       conv_param.dilation((num_dilation_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Special case: im2col is the identity for 1x1 convolution with stride 1</span></span><br><span class="line">  <span class="comment">// and no padding, so flag for skipping the buffer and transformation.</span></span><br><span class="line">  is_1x1_ = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</span><br><span class="line">    is_1x1_ &amp;=</span><br><span class="line">        kernel_shape_data[i] == <span class="number">1</span> &amp;&amp; stride_data[i] == <span class="number">1</span> &amp;&amp; pad_data[i] == <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (!is_1x1_) &#123; <span class="keyword">break</span>; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Configure output channels and groups.</span></span><br><span class="line">  <span class="comment">// channel_axis_ default = 1, channels_ = ?</span></span><br><span class="line">  channels_ = bottom[<span class="number">0</span>]-&gt;shape(channel_axis_);</span><br><span class="line">  <span class="comment">// defined in train_test.prototxt</span></span><br><span class="line">  num_output_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().num_output();</span><br><span class="line">  CHECK_GT(num_output_, <span class="number">0</span>);</span><br><span class="line">  <span class="comment">// default = 1, but what is it?</span></span><br><span class="line">  group_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().group();</span><br><span class="line">  CHECK_EQ(channels_ % group_, <span class="number">0</span>);</span><br><span class="line">  CHECK_EQ(num_output_ % group_, <span class="number">0</span>)</span><br><span class="line">      &lt;&lt; <span class="string">"Number of output should be multiples of group."</span>;</span><br><span class="line">  <span class="keyword">if</span> (reverse_dimensions()) &#123; <span class="comment">// deconv:true, conv:false.</span></span><br><span class="line">    conv_out_channels_ = channels_;</span><br><span class="line">    conv_in_channels_ = num_output_;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    conv_out_channels_ = num_output_;</span><br><span class="line">    conv_in_channels_ = channels_;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Handle the parameters: weights and biases.</span></span><br><span class="line">  <span class="comment">// - blobs_[0] holds the filter weights</span></span><br><span class="line">  <span class="comment">// - blobs_[1] holds the biases (optional)</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weight_shape(<span class="number">2</span>);</span><br><span class="line">  weight_shape[<span class="number">0</span>] = conv_out_channels_;</span><br><span class="line">  weight_shape[<span class="number">1</span>] = conv_in_channels_ / group_;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</span><br><span class="line">    weight_shape.push_back(kernel_shape_data[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  bias_term_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().bias_term();</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bias_shape(bias_term_, num_output_);</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    CHECK_EQ(<span class="number">1</span> + bias_term_, <span class="keyword">this</span>-&gt;blobs_.size())</span><br><span class="line">        &lt;&lt; <span class="string">"Incorrect number of weight blobs."</span>;</span><br><span class="line">    <span class="keyword">if</span> (weight_shape != <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;shape()) &#123;</span><br><span class="line">      Blob&lt;Dtype&gt; weight_shaped_blob(weight_shape);</span><br><span class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Incorrect weight shape: expected shape "</span></span><br><span class="line">          &lt;&lt; weight_shaped_blob.shape_string() &lt;&lt; <span class="string">"; instead, shape was "</span></span><br><span class="line">          &lt;&lt; <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;shape_string();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (bias_term_ &amp;&amp; bias_shape != <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;shape()) &#123;</span><br><span class="line">      Blob&lt;Dtype&gt; bias_shaped_blob(bias_shape);</span><br><span class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Incorrect bias shape: expected shape "</span></span><br><span class="line">          &lt;&lt; bias_shaped_blob.shape_string() &lt;&lt; <span class="string">"; instead, shape was "</span></span><br><span class="line">          &lt;&lt; <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;shape_string();</span><br><span class="line">    &#125;</span><br><span class="line">    LOG(INFO) &lt;&lt; <span class="string">"Skipping parameter initialization"</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (bias_term_) &#123;</span><br><span class="line">      <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">2</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Initialize and fill the weights:</span></span><br><span class="line">    <span class="comment">// output channels x input channels per-group x kernel height x kernel width</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].reset(new Blob&lt;Dtype&gt;(weight_shape));</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;Filler&lt;Dtype&gt; &gt; weight_filler(GetFiller&lt;Dtype&gt;(</span><br><span class="line">        this-&gt;layer_param_.convolution_param().weight_filler()));</span><br><span class="line">    weight_filler-&gt;Fill(<span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].get());</span><br><span class="line">    <span class="comment">// If necessary, initialize and fill the biases.</span></span><br><span class="line">    <span class="keyword">if</span> (bias_term_) &#123;</span><br><span class="line">      <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].reset(new Blob&lt;Dtype&gt;(bias_shape));</span><br><span class="line">      <span class="built_in">shared_ptr</span>&lt;Filler&lt;Dtype&gt; &gt; bias_filler(GetFiller&lt;Dtype&gt;(</span><br><span class="line">          this-&gt;layer_param_.convolution_param().bias_filler()));</span><br><span class="line">      bias_filler-&gt;Fill(<span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  kernel_dim_ = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;count(<span class="number">1</span>);</span><br><span class="line">  weight_offset_ = conv_out_channels_ * kernel_dim_ / group_;</span><br><span class="line">  <span class="comment">// Propagate gradients to the parameters (as directed by backward pass).</span></span><br><span class="line">  <span class="keyword">this</span>-&gt;param_propagate_down_.resize(<span class="keyword">this</span>-&gt;blobs_.size(), <span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>唉，代码解释就先写到这吧，因为我发现了另外一个有趣的事情：</p>
<p>在这个版本中<a href="https://github.com/BVLC/caffe/tree/87eff5ff04c32c7fc35a7e0f8ee36a1a72278c1e" target="_blank" rel="external">87eff5f</a>，有nd pooling的代码，而在最新版本中(avril 25, 2016)，有nd conv的代码，于是，我把[87eff5f]的pooling代码粘贴过来了，改了caffe.proto，还改了一些set_pad=&gt;add_pad，都是根据编译错误来改的。然后就能运行了。。。。。只不过，还是发现两个GPU有时并不同时工作。我降低了batch_size，发现当GPU内存较小时（3G以下或者4G以下），两个GPU就能同时达到99%的工作效率，速度大概是10s 20次迭代。alexnet大概是4.8s 20次。算是不错的了。另外，cudnn并不支持nd conv，但是好像支持nd pooling，不是很明白，所以engine要选caffe。pooling的代码我改过，直接让它用caffe engine的，不然编译还是运行的时候就会报错。</p>
<p>真是奇怪啊，caffe之前就把nd pooling写了（2015年5月）；但是好像11月的时候还是之前经历了一次结构上的大调整，把layer的hpp单独提出来了，之前conv和pool等都是在vision_layer.hpp里定义的。结果调整之后，pooling就变成很早之前的版本了。</p>
<p>接下来就是要看怎么把数据传进去。之前一直用的dummydatalayer。</p>
<p>SoftmaxWithLoss是沿着一个axis来进行softmax的，如果是semantic segmentation，可以把label的dimensions设置为227<em>227，然后fc8最后的输出也是227</em>227。考虑一下是否可行。</p>
<p>要输入类似video的连续图片，我觉得重写tools/convert_imageset就好了吧。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/13/2016-04-25-caffe-code-reading-translating-official-tutorial/" itemprop="url">
                  Caffe Code Reading - translating official tutorial
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-13T01:39:01+02:00" content="2016-05-13">
              2016-05-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/13/2016-04-25-caffe-code-reading-translating-official-tutorial/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/13/2016-04-25-caffe-code-reading-translating-official-tutorial/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Caffe的代码需要好好看看。。</p>
<hr>
<p>先从官网的<a href="http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html" target="_blank" rel="external">tutorial</a>开始吧。主要还是翻译和总结其中重要的点。</p>
<h1 id="Blob-storage-and-communication"><a href="#Blob-storage-and-communication" class="headerlink" title="Blob storage and communication"></a><strong>Blob storage and communication</strong></h1><blockquote>
<p>Mathematically, a blob is an N-dimensional array stored in a C-contiguous fasion.</p>
</blockquote>
<p>不管是batches of images, 还是model parameters，或者导数，都是用blob来存和communicates的。</p>
<blockquote>
<p>Blobs conceal the computational and mental overhead of mixed CPU/GPU operation by synchronizing from the CPU host to the GPU device as needed. Memory on the host and device is allocated on demand (lazily) for efficient memory usage.</p>
</blockquote>
<p>对于batches of image data，普通的blob维度就是batch_zise(number) N，channel K, 高度H和宽度W。blob中at index (n, k, h, w)的位置就是(（n <em> K + k）</em> H + h ) * W + w。</p>
<p>对于fc，用2D的blob (shape(N,D))。D哪来的？是啥啊？我觉得是fc里的neuron的个数。后面有个例子，1000的output channels，1024的input channels， the parameter blob is 1000 × 1024。</p>
<h1 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a><strong>Implementation Details</strong></h1><blockquote>
<p>a Blob stores two chunks of memories, <em>data</em> and <em>diff</em></p>
</blockquote>
<p>一个是普通data，一个是算gradient的。</p>
<p>访问数据的方式有两种，const和mutable：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(similar for gpu and diff)</p>
</blockquote>
<p>也就是说，这个是普通的用于cpu计算的data，gpu的单独另存一个地方？</p>
<p>所以，Blob用一个SyncedMem类来进行同步CPU和GPU之间的通信。</p>
<blockquote>
<p>A rule of thumb is, always use the const call if you do not want to change the values, and never store the pointers in your own object. Every time you work on a blob, call the functions to get the pointers, as the SyncedMem will need this to figure out when to copy data.</p>
</blockquote>
<p>现实中，当有GPUs的时候，我们从硬盘中读取数据存到blob里（CPU），然后调用GPU的kernel做计算，然后把数据传给下一层，忽略了low-level的细节，而保持很高的performance（翻译不来了）。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assuming that data are on the CPU initially, and we have a blob.</span></span><br><span class="line"><span class="keyword">const</span> Dtype* foo;</span><br><span class="line">Dtype* bar;</span><br><span class="line">foo = blob.gpu_data(); <span class="comment">// data copied cpu-&gt;gpu.</span></span><br><span class="line">foo = blob.cpu_data(); <span class="comment">// no data copied since both have up-to-date contents.</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// no data copied.</span></span><br><span class="line"><span class="comment">// ... some operations ...</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// no data copied when we are still on GPU.</span></span><br><span class="line">foo = blob.cpu_data(); <span class="comment">// data copied gpu-&gt;cpu, since the gpu side has modified the data</span></span><br><span class="line">foo = blob.gpu_data(); <span class="comment">// no data copied since both have up-to-date contents</span></span><br><span class="line">bar = blob.mutable_cpu_data(); <span class="comment">// still no data copied.</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// data copied cpu-&gt;gpu.</span></span><br><span class="line">bar = blob.mutable_cpu_data(); <span class="comment">// data copied gpu-&gt;cpu.</span></span><br></pre></td></tr></table></figure>
<p>也就是说，要避免data copy，就尽量在GPU上把operations都一次性做完了再传回cpu。</p>
<h1 id="Layer-computation-and-connections-vision-layer"><a href="#Layer-computation-and-connections-vision-layer" class="headerlink" title="Layer computation and connections - vision layer"></a><strong>Layer computation and connections - vision layer</strong></h1><p>这跳到<a href="http://caffe.berkeleyvision.org/tutorial/layers.html" target="_blank" rel="external">layers</a>。</p>
<blockquote>
<p>Header: ./include/caffe/vision_layers.hpp</p>
</blockquote>
<p>作为图片的最重要的layer群，但是caffe把它认为是一个特殊的layer。因为它的2D信息（高和宽），几乎所有的操作都是针对图片上的某些区域来进行的（比如convolution，pooling等）；而其它的layer只是将输入当做一个很大的vector，维度为chw（或者khw）。</p>
<p><strong>Convolution</strong></p>
<blockquote>
<p>Layer type: Convolution</p>
<p>CPU implementation: ./src/caffe/layers/convolution_layer.cpp</p>
<p>CUDA GPU implementation: ./src/caffe/layers/convolution_layer.cu</p>
<p>Parameters (ConvolutionParameter convolution_param)</p>
</blockquote>
<p><strong>Pooling</strong></p>
<blockquote>
<p>Layer type: Pooling</p>
<p>CPU implementation: ./src/caffe/layers/pooling_layer.cpp</p>
<p>CUDA GPU implementation: ./src/caffe/layers/pooling_layer.cu</p>
</blockquote>
<p><strong>Local Response Normalization</strong></p>
<blockquote>
<p>Layer type: LRN</p>
<p>CPU Implementation: ./src/caffe/layers/lrn_layer.cpp</p>
<p>CUDA GPU Implementation: ./src/caffe/layers/lrn_layer.cu</p>
</blockquote>
<p><strong>im2col</strong></p>
<blockquote>
<p>Im2col is a helper for doing the image-to-column transformation that you most likely do not need to know about. This is used in Caffe’s original convolution to do matrix multiplication by laying out all patches into a matrix.</p>
</blockquote>
<p>see <a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="external">this</a>.</p>
<p>发现这上面其实没有太多代码的内容。</p>
<p>然后代码我觉得从Layer开始看起比较好。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/13/2016-04-22-ReadingPaper-Fast-RCNN/" itemprop="url">
                  Fast R-CNN#PaperReading#
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-13T01:38:35+02:00" content="2016-05-13">
              2016-05-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/13/2016-04-22-ReadingPaper-Fast-RCNN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/13/2016-04-22-ReadingPaper-Fast-RCNN/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>KeyWords</strong>: Region proposal, SPP, Object Detection</p>
<hr>
<p><strong>Paper Author</strong>: Ross Girshick <strong>Research Institu</strong>: Microsoft Reasearch</p>
<p><strong>Database</strong>:  </p>
<p><strong>GPU &amp; timing</strong>: one K40 GPU</p>
<p><strong>Paper Link</strong>: <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html" target="_blank" rel="external">cvpr15</a></p>
<hr>
<h1 id="Abstract-amp-Introduction"><a href="#Abstract-amp-Introduction" class="headerlink" title="Abstract&amp;Introduction"></a><strong>Abstract&amp;Introduction</strong></h1><p>用VGG16，训练速度比R-CNN快16倍，测试速度快213倍，并且在PASCAL VOC 2012上的mAP要高。和SPP比，3倍训练速度，10倍测试速度，更准确。</p>
<p>文中说R-CNN的训练是分多个阶段的：</p>
<ul>
<li>fine-tuning ConvNet来提取特征</li>
<li>SVM分类训练</li>
<li>regressor对bounding box进行更准确的预测。</li>
</ul>
<p><strong>问题1</strong>：如果三个阶段是分离的，ConvNet是怎么训练的？ 原文如下，但是和什么比较呢？我看RCNN原文2.3中写得有点怪，pre-trained之后的CNN把最后一层fc去掉，换成21-way的classifier，然后fine-tuning。但是最后还是extract features (4096维的吧)，然后用SVM分类，我的理解是，fast RCNN中说的第一个阶段，就是fine-tuning CNN的阶段，然后把21-way的那层去掉，换成一个SVM。所以这样又要第二个阶段。作者的理解应该不会错的，毕竟两篇都是他写的。。。</p>
<blockquote>
<p>using log loss</p>
</blockquote>
<p><strong>20160503加</strong></p>
<p>pre-train是按照image-level annotations来训练的，是个正常的分类网络。然后他再把最后一层换成了21-way的输出，然后再用voc的数据来fine-tuning这个cnn；最后再把前一层4096的特征提出来，用svm来分类。最后对bbox的回归也是要单独训练。</p>
<hr>
<p>SPP可以sharing computation。具体见SPPNet原文。SPP的缺点：</p>
<blockquote>
<ul>
<li>和R-CNN一样，训练也是分离的 (multi-stage pipeline)</li>
<li>提取的特征要写入到硬盘</li>
<li>不能更新SPP层之前的卷积层</li>
</ul>
</blockquote>
<p>所以，fast R-CNN的几个优点就是训练是single-stage using a multi-task loss，可以更新所有的网络层，不需要写入硬盘。</p>
<hr>
<h1 id="The-Rest"><a href="#The-Rest" class="headerlink" title="The Rest"></a><strong>The Rest</strong></h1><p>类似SPP，在所有conv layer之后，加上了一个ROI pooling layer。在图片中的ROI，会先在conv feature map上找到对应的ROI,大小hw，然后将hw大小的ROI推进到HW的grid里（VGG16中是7x7），所以有49个sub-windows，每个sub-windows的大小大约在h/H x w/W。最后的做pooling的是每一个sub-windows，输出的尺寸即是256×49（256是上一层的channels，有可能是512，忘了）。所以文章中说</p>
<blockquote>
<p>The ROI layer is simply the special-case of the spp layer used in SPPnets in which there is only one pyramid level.</p>
</blockquote>
<p>fast rcnn用的只有一层pyramid level。</p>
<p>文章2.2中说</p>
<blockquote>
<p>when a pre-trained network initializes a fast r-cnn network, it undergoes three transformations.</p>
<ul>
<li>第一个是最后一层max pooling layer被替换成了ROI pooling layer。虽然这里没有参数，但是需要调整HW来使其与第一层fc compatible。</li>
<li>第二个是最后一层的fc和softmax被替换成了自己要做的。</li>
<li>第三个是输入变了。</li>
</ul>
</blockquote>
<p>那</p>
<p><strong>问题2</strong>：变化的第一个，VGG16中，有对应关系，直接这样拿过来fine-tuning，行么？</p>
<p>在2.3中，作者解释了为什么SPP和RCNN都比较慢的原因：当每个training sample都是来自不同的图片时，bp通过spp的效率特别低，因为每个ROI可能会特别大，（spp每次都要计算来自不同的图片中的ROI，这样并没有发挥share的效果，所以特别慢）。而本文中，每个batch中的样本是hierarchically sampled，每N张图片中，sample R/N个ROI，例子N=2， R=128，这样比RCNN和SPP快64倍roughly。</p>
<p>作者也提到，这样每个batch里的ROI都correlated，会影响收敛速度。但是实际上并没有这个问题，相反用的迭代次数反而比RCNN少。</p>
<p>两个loss function，加起来。regressor用的不是L2，变了一些。cls还是softmax, log loss。</p>
<p>bp through roi pooling layer。。。</p>
<p>初始化参数，用来分类的fc和bbox的fc参数初始化为平均值为0，均方差为0.01和0.001的高斯分布，global lr=0.001，weights lr = 1, bias lr = 2。小数据库（voc07,12）时，30k iteration,然后将lr降为0.0001在训练10k次。大点的数据库，文中没看到。</p>
<p>4.5中，作者认为浅层的的卷积层可以不fine-tuning，但是稍微深层一点的（9/13开始）还是需要，如果只从fc开始，mAP下降了5.5%。还有，</p>
<blockquote>
<p>In the smaller networks we find that conv1 is generic and task independent.</p>
</blockquote>
<p>这篇就先写到这吧。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/13/2016-04-18-notes-of-problems-when-installing-opencv-and-caffe/" itemprop="url">
                  Notes of problems when installing opencv and caffe
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-13T01:38:13+02:00" content="2016-05-13">
              2016-05-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/13/2016-04-18-notes-of-problems-when-installing-opencv-and-caffe/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/13/2016-04-18-notes-of-problems-when-installing-opencv-and-caffe/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Some problems I’ve met when I was installing opencv and caffe.</p>
<h1 id="OpenCV-Install-Problems"><a href="#OpenCV-Install-Problems" class="headerlink" title="OpenCV Install Problems"></a>OpenCV Install Problems</h1><p>Because of some problems, I have to install two versions of opencv (2.4.12 and 3.1.0, at 18 Avril 2016). But when I compile the source of opencv, I have to point out the install dir of opencv. At first, I use opencv2.4.10 and opencv 3.1.0, but when the program tries to find opencv, the dir is not correct. After changing to opencv2412 and opencv310, there is no problem any more.</p>
<p>I just have to add </p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(CMAKE_PREFIX_PATH <span class="string">"/usr/local/opencv2.4"</span>)</span><br><span class="line"><span class="keyword">find_package</span>( OpenCV <span class="number">2.4</span> REQUIRED )</span><br></pre></td></tr></table></figure>
<p>in the CMakeLists.txt.</p>
<p>And before this, there was a problem when I compile 2.4.12. I want to compile with contrib but with 2.4.12, there is some problem with the lib dnn so I have to turn it off. No problem when compiling with 3.1.0.</p>
<p>You haven’t put the shared library in a location where the loader can find it. look inside the /usr/local/opencv and /usr/local/opencv2 folders and see if either of them contains any shared libraries (files beginning in lib and usually ending in .so). when you find them, create a file called /etc/ld.so.conf.d/opencv.conf and write to it the paths to the folders where the libraries are stored, one per line. Then run</p>
<p>sudo ldconfig -v<br>for example, if the libraries were stored under /usr/local/opencv/libopencv_core.so.2.4 then I would write this to my opencv.conf file:</p>
<p>/usr/local/opencv/<br>If you can’t find the libraries, try running</p>
<p>sudo updatedb &amp;&amp; locate libopencv_core.so.2.4<br>in a shell. You don’t need to run updatedb if you’ve rebooted since compiling OpenCV.</p>
<h1 id="Caffe-Install-Problems"><a href="#Caffe-Install-Problems" class="headerlink" title="Caffe Install Problems"></a>Caffe Install Problems</h1><p>Following with <a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="external">Installation for Caffe</a>, no pb if I install opencv in the default dir. But after changing the install dir to /usr/local/opencv310, there is pb in compilation and I don’t know how to add the opencv dir in the makefile.config. I searched a little and I found the ways annoying so I use cmake instead. And with cmake (mkdir build), the directory didn’t change comparing with makefile.config.</p>
<p><a href="https://github.com/BVLC/caffe/tree/458928a3bc1ee94e5f12bb254a5de819c449fc0a" target="_blank" rel="external">This caffe version 458928a</a> (18 Avril 2016) is compatible with cudnn v4, not v5. </p>
<p>for pycaffe, see <a href="https://github.com/tiangolo/caffe/blob/ubuntu-tutorial-b/docs/install_apt2.md" target="_blank" rel="external">this</a> and install gfortran before installing the requirements.txt</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">sudo</span> apt-<span class="meta">get</span> install libatlas-<span class="keyword">base-dev </span>gfortran</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="holyseven" />
          <p class="site-author-name" itemprop="name">holyseven</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">8</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">holyseven</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"holyseven"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  





  
  
  

  

  

</body>
</html>
